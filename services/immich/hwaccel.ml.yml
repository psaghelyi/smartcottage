# Configurations for hardware-accelerated machine learning

# If using Unraid or another platform that doesn't allow multiple Compose files,
# you can inline the config for a backend by copying its contents
# into the immich-machine-learning service in the docker-compose.yml file.

# See https://immich.app/docs/features/ml-hardware-acceleration for more info on using hardware acceleration.

services:
  cpu: {}

  openvino:
    devices:
      - /dev/dri:/dev/dri
    environment:
      - OPENVINO_DEVICE=GPU
      - OPENVINO_CACHE_DIR=/cache

  cuda:
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities:
                - gpu
                - compute

  armnn: {}

  openvino-wsl:
    # use this for OpenVINO if you're running Immich in WSL2
    devices:
      - /dev/dri:/dev/dri
      - /dev/dxg:/dev/dxg
    volumes:
      - /usr/lib/wsl:/usr/lib/wsl
    environment:
      - OPENVINO_DEVICE=GPU
      - OPENVINO_CACHE_DIR=/cache
      - LIBVA_DRIVER_NAME=d3d12